{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "stylegGAN2_colab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87fN-d1-N9s0"
      },
      "source": [
        "# StyleGAN2-ADA\n",
        "\n",
        "참조한 링크.\n",
        "\n",
        "- https://colab.research.google.com/github/dvschultz/ml-art-colabs/blob/master/Stylegan2_ada_Custom_Training.ipynb\n",
        "- https://colab.research.google.com/github/Hephyrius/Stylegan2-Ada-Google-Colab-Starter-Notebook/blob/main/Stylegan2_Ada_Colab_Starter.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFB64iL9BzBd"
      },
      "source": [
        "StyleGAN2-ADA 모델은 텐서플로우 1 버전에서만 작동합니다. 아래 셀을 실행해서 1 버전을 사용할 수 있도록 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbyUpDO4OLiU",
        "outputId": "64a9da44-492c-44fa-ed23-d6d0d4b2c878"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91s4Bdk2CAJi"
      },
      "source": [
        "GPU가 실행 되는지 확인 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KiBLUTr7OVoB",
        "outputId": "a22175be-72c9-441c-de19-f3e4c6ea629b"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Nov 15 08:26:37 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoXcz7cMsmi8",
        "outputId": "bb0058ba-a72e-41dc-b360-3b5fc9a57fb2"
      },
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 27.3 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhGEeUh8CEMc"
      },
      "source": [
        "구글 드라이브와 연동해서 데이터를 가져옵니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F46YSLmVOWWa",
        "outputId": "c8282f4f-eb95-4dea-e7c6-c0fe2be4481b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1jG7Cx1RWKY"
      },
      "source": [
        "깃에서 StyleGAN2-ADA을 복사해 옵니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVtHBVKbOmKl",
        "outputId": "50cdc5ba-9ab8-469a-f47d-83396a71e700"
      },
      "source": [
        "# clone git code\n",
        "%cd /content/\n",
        "!git clone https://github.com/KyungRyeolBaek/stylegan2-ada.git\n",
        "!mkdir downloads\n",
        "!mkdir datasets"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'stylegan2-ada'...\n",
            "remote: Enumerating objects: 364, done.\u001b[K\n",
            "remote: Total 364 (delta 0), reused 0 (delta 0), pack-reused 364\u001b[K\n",
            "Receiving objects: 100% (364/364), 56.16 MiB | 33.67 MiB/s, done.\n",
            "Resolving deltas: 100% (200/200), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffxQiIkQRTGO"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oa5ILnoJXNXq"
      },
      "source": [
        "구글 드라이브에 훈련 할 이미지를 zip 파일로 저장해 놓습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w637CVdoO3du"
      },
      "source": [
        "dataset_name = \"best_images\""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NKBXJsUC1Cc"
      },
      "source": [
        "노트북으로 데이터셋 압축파일을 불러와서 압축을 풉니다. 왼쪽 Files에서 확인할 수 있습니다.시간이 조금 소요 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tdy-VknQbkW",
        "outputId": "3c1d957b-188e-4fa7-fd1d-47b0dc57d259"
      },
      "source": [
        "import zipfile\n",
        "\n",
        "path = \"/content/drive/MyDrive/content/datasets/best_images/\" # 압축 파일이 저장되어 있는 파일의 경로\n",
        "local_path = \"/content/best_images/\" # 이미지 저장 할 경로\n",
        "file_name = path + dataset_name + \".zip\"\n",
        "with zipfile.ZipFile(file_name, 'r') as zip:\n",
        "   print('파일 압축 해제 중...') \n",
        "   zip.extractall(local_path)\n",
        "   print('끝!')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "파일 압축 해제 중...\n",
            "끝!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxtOLRCvDULH"
      },
      "source": [
        "불러온 이미지 데이터셋들을 tensorflow에서 사용할 수 있는 데이터셋 포멧인 tfrecords 형태로 바꿔줍니다. 데이터셋의 크기에 소요되는 시간이 다를 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlS7sB4xQtmC",
        "outputId": "7f7a8aed-9f54-4f45-d642-afcf02113926"
      },
      "source": [
        "%cd /content/\n",
        "\n",
        "# 이미지 경로 설정\n",
        "dataset_path = \"/content/best_images/best_images\" # 저장된 이미지가 있는 폴더\n",
        "\n",
        "# 빈 파일을 따로 생성 할 필요 없이 자동으로 생성 됩니다.\n",
        "%cd /content/stylegan2-ada\n",
        "!python dataset_tool.py create_from_images /content/datasets/{dataset_name} {dataset_path} # 이미지 처리 후 저장 할 곳, 저장된 이미지가 있는 경로"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/stylegan2-ada\n",
            "Loading images from \"/content/best_images/best_images\"\n",
            "Creating dataset \"/content/datasets/best_images\"\n",
            "dataset_tool.py:97: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
            "  'data': tf.train.Feature(bytes_list=tf.train.BytesList(value=[quant.tostring()]))}))\n",
            "Added 159 images.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyhe_tggRPnq"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrikBYS-VOnO"
      },
      "source": [
        "아래 파라미터에 대한 설명을 보고 수정 가능 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPPicfQXQ_PZ",
        "outputId": "1c206287-a249-4b10-89a5-c14a09a12e7b"
      },
      "source": [
        "%cd /content/stylegan2-ada\n",
        "!python train.py --help"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/stylegan2-ada\n",
            "usage: train.py [-h] --outdir DIR [--gpus INT] [--snap INT] [--seed INT] [-n]\n",
            "                --data PATH [--res INT] [--mirror BOOL] [--mirrory BOOL]\n",
            "                [--use-raw BOOL] [--metrics LIST] [--metricdata PATH]\n",
            "                [--cfg {auto,11gb-gpu,11gb-gpu-complex,24gb-gpu,24gb-gpu-complex,48gb-gpu,48gb-2gpu,stylegan2,paper256,paper512,paper1024,cifar,cifarbaseline,aydao}]\n",
            "                [--lrate FLOAT] [--ttur BOOL] [--gamma FLOAT] [--nkimg INT]\n",
            "                [--kimg INT] [--topk FLOAT] [--aug {noaug,ada,fixed,adarv}]\n",
            "                [--p FLOAT] [--target TARGET] [--initstrength INITSTRENGTH]\n",
            "                [--augpipe {blit,geom,color,filter,noise,cutout,bg,bgc,bgcf,bgcfn,bgcfnc}]\n",
            "                [--cmethod {nocmethod,bcr,zcr,pagan,wgangp,auxrot,spectralnorm,shallowmap,adropout}]\n",
            "                [--dcap FLOAT] [--resume RESUME] [--freezed INT]\n",
            "\n",
            "Train a GAN using the techniques described in the paper\n",
            "\"Training Generative Adversarial Networks with Limited Data\".\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "\n",
            "general options:\n",
            "  --outdir DIR          Where to save the results (required)\n",
            "  --gpus INT            Number of GPUs to use (default: 1 gpu)\n",
            "  --snap INT            Snapshot interval (default: 50 ticks)\n",
            "  --seed INT            Random seed (default: 1000)\n",
            "  -n, --dry-run         Print training options and exit\n",
            "\n",
            "training dataset:\n",
            "  --data PATH           Training dataset path (required)\n",
            "  --res INT             Dataset resolution (default: highest available)\n",
            "  --mirror BOOL         Augment dataset with x-flips (default: false)\n",
            "  --mirrory BOOL        Augment dataset with y-flips (default: false)\n",
            "  --use-raw BOOL        Use raw image dataset, i.e. created from\n",
            "                        create_from_images_raw (default: False)\n",
            "\n",
            "metrics:\n",
            "  --metrics LIST        Comma-separated list or \"none\" (default: fid50k_full)\n",
            "  --metricdata PATH     Dataset to evaluate metrics against (optional)\n",
            "\n",
            "base config:\n",
            "  --cfg {auto,11gb-gpu,11gb-gpu-complex,24gb-gpu,24gb-gpu-complex,48gb-gpu,48gb-2gpu,stylegan2,paper256,paper512,paper1024,cifar,cifarbaseline,aydao}\n",
            "                        Base config (default: auto)\n",
            "  --lrate FLOAT         Override learning rate\n",
            "  --ttur BOOL           Use Two Time-Scale Update Rule (double learning rate\n",
            "                        for discriminator) (default: false)\n",
            "  --gamma FLOAT         Override R1 gamma\n",
            "  --nkimg INT           Override starting count\n",
            "  --kimg INT            Override training duration\n",
            "  --topk FLOAT          utilize top-k training\n",
            "\n",
            "discriminator augmentation:\n",
            "  --aug {noaug,ada,fixed,adarv}\n",
            "                        Augmentation mode (default: ada)\n",
            "  --p FLOAT             Specify augmentation probability for --aug=fixed\n",
            "  --target TARGET       Override ADA target for --aug=ada and --aug=adarv\n",
            "  --initstrength INITSTRENGTH\n",
            "                        Override ADA strength at start\n",
            "  --augpipe {blit,geom,color,filter,noise,cutout,bg,bgc,bgcf,bgcfn,bgcfnc}\n",
            "                        Augmentation pipeline (default: bgc)\n",
            "\n",
            "comparison methods:\n",
            "  --cmethod {nocmethod,bcr,zcr,pagan,wgangp,auxrot,spectralnorm,shallowmap,adropout}\n",
            "                        Comparison method (default: nocmethod)\n",
            "  --dcap FLOAT          Multiplier for discriminator capacity\n",
            "\n",
            "transfer learning:\n",
            "  --resume RESUME       Resume from network pickle (default: noresume)\n",
            "  --freezed INT         Freeze-D (default: 0 discriminator layers)\n",
            "\n",
            "examples:\n",
            "\n",
            "  # Train custom dataset using 1 GPU.\n",
            "  python train.py --outdir=~/training-runs --gpus=1 --data=~/datasets/custom\n",
            "\n",
            "  # Train class-conditional CIFAR-10 using 2 GPUs.\n",
            "  python train.py --outdir=~/training-runs --gpus=2 --data=~/datasets/cifar10c \\\n",
            "      --cfg=cifar\n",
            "\n",
            "  # Transfer learn MetFaces from FFHQ using 4 GPUs.\n",
            "  python train.py --outdir=~/training-runs --gpus=4 --data=~/datasets/metfaces \\\n",
            "      --cfg=paper1024 --mirror=1 --resume=ffhq1024 --snap=10\n",
            "\n",
            "  # Reproduce original StyleGAN2 config F.\n",
            "  python train.py --outdir=~/training-runs --gpus=8 --data=~/datasets/ffhq \\\n",
            "      --cfg=stylegan2 --res=1024 --mirror=1 --aug=noaug\n",
            "\n",
            "available base configs (--cfg):\n",
            "  auto           Automatically select reasonable defaults based on resolution\n",
            "                 and GPU count. Good starting point for new datasets.\n",
            "  stylegan2      Reproduce results for StyleGAN2 config F at 1024x1024.\n",
            "  paper256       Reproduce results for FFHQ and LSUN Cat at 256x256.\n",
            "  paper512       Reproduce results for BreCaHAD and AFHQ at 512x512.\n",
            "  paper1024      Reproduce results for MetFaces at 1024x1024.\n",
            "  cifar          Reproduce results for CIFAR-10 (tuned configuration).\n",
            "  cifarbaseline  Reproduce results for CIFAR-10 (baseline configuration).\n",
            "\n",
            "transfer learning source networks (--resume):\n",
            "  ffhq256        FFHQ trained at 256x256 resolution.\n",
            "  ffhq512        FFHQ trained at 512x512 resolution.\n",
            "  ffhq1024       FFHQ trained at 1024x1024 resolution.\n",
            "  celebahq256    CelebA-HQ trained at 256x256 resolution.\n",
            "  lsundog256     LSUN Dog trained at 256x256 resolution.\n",
            "  afhqcat512     AFHQ Cat trained at 512x512 resolution.\n",
            "  afhqdog512     AFHQ Dog trained at 512x512 resolution.\n",
            "  afhqwild512    AFHQ Wild trained at 512x512 resolution.\n",
            "  brecahad512    BreCaHAD trained at 512x512 resolution.\n",
            "  cifar10        CIFAR10 trained at 32x32 resolution.\n",
            "  metfaces512    MetFaces trained at 512x512 resolution.\n",
            "  <path or URL>  Custom network pickle.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2HvSTYCad78"
      },
      "source": [
        "아래 셀이 훈련 시작 입니다. \n",
        "\n",
        "## 훈련 중에는 colab이 꺼질 시 처음부터 다시 실행 해야 합니다.\n",
        "\n",
        "pkl로 학습을 진행하고 싶은 경우는 resum_from의 경로를 pkl 파일로 변경 해주면 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p70HPLuxa2aA",
        "outputId": "a046ce7b-7d6a-421c-c4c8-8ae102d8a7a2"
      },
      "source": [
        "# output directory\n",
        "output_dir = '/content/drive/MyDrive/Colab\\ Notebooks/results/' + dataset_name + \"/\"\n",
        "\n",
        "# config\n",
        "config = \"auto\"\n",
        "\n",
        "# gamma\n",
        "gamma = 1\n",
        "\n",
        "#how often should the model generate samples and a .pkl file\n",
        "snapshot_count = 1\n",
        "\n",
        "#should the images be mirrored left to right?\n",
        "mirrored = True\n",
        "#should the images be mirrored top to bottom?\n",
        "mirroredY = False\n",
        "\n",
        "#metrics? \n",
        "metric_list = None\n",
        "#\n",
        "# this is the most important cell to update\n",
        "#\n",
        "# running it for the first time? set it to ffhq(+resolution)\n",
        "# resuming? get the path to your latest .pkl file and use that\n",
        "resume_from = \"ffhq1024\"\n",
        "\n",
        "# make sure there is no space in the resume path. if there is any, use a backslash character to escape\n",
        "# resume_from = '/content/drive/MyDrive/Colab\\ Notebooks/results/train787/00003-train787-mirror-stylegan2-gamma1-resumeffhq1024/network-snapshot-000143.pkl'\n",
        "\n",
        "#don't edit this unless you know what you're doing :)\n",
        "!python train.py --outdir={output_dir} \\\n",
        "                 --cfg={config} \\\n",
        "                 --snap={snapshot_count} \\\n",
        "                 --data=/content/datasets/{dataset_name} \\\n",
        "                 --mirror={mirrored} --mirrory={mirroredY} \\\n",
        "                 --gamma={gamma} \\\n",
        "                 --metrics={metric_list} \\\n",
        "                 --resume={resume_from}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tcmalloc: large alloc 4294967296 bytes == 0x55e487686000 @  0x7fcaa7ae3001 0x7fcaa4ce654f 0x7fcaa4d36b58 0x7fcaa4d3ab17 0x7fcaa4dd9203 0x55e4806d1544 0x55e4806d1240 0x55e480745627 0x55e48073fced 0x55e4806d348c 0x55e480714159 0x55e4807110a4 0x55e4806d1d49 0x55e48074594f 0x55e48073f9ee 0x55e480611e2b 0x55e480741fe4 0x55e48073f9ee 0x55e480611e2b 0x55e480741fe4 0x55e48073fced 0x55e480611e2b 0x55e480741fe4 0x55e4806d2afa 0x55e480740915 0x55e48073f9ee 0x55e48073f6f3 0x55e4808094c2 0x55e48080983d 0x55e4808096e6 0x55e4807e1163\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x55e587686000 @  0x7fcaa7ae11e7 0x7fcaa4ce646e 0x7fcaa4d36c7b 0x7fcaa4d3735f 0x7fcaa4dd9103 0x55e4806d1544 0x55e4806d1240 0x55e480745627 0x55e48073f9ee 0x55e4806d2bda 0x55e480741737 0x55e48073f9ee 0x55e4806d2bda 0x55e480741737 0x55e48073f9ee 0x55e4806d2bda 0x55e480741737 0x55e4806d2afa 0x55e480740915 0x55e48073f9ee 0x55e4806d2bda 0x55e480744d00 0x55e48073f9ee 0x55e4806d2bda 0x55e480741737 0x55e48073fced 0x55e4806d348c 0x55e480714159 0x55e4807110a4 0x55e4806d1d49 0x55e48074594f\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x55e68882e000 @  0x7fcaa7ae11e7 0x7fcaa4ce646e 0x7fcaa4d36c7b 0x7fcaa4d3735f 0x7fca506e6235 0x7fca50069792 0x7fca50069d42 0x7fca50022aee 0x55e4806d1437 0x55e4806d1240 0x55e4807450f3 0x55e4806d2afa 0x55e480740c0d 0x55e48073fced 0x55e480611eb0 0x55e480741fe4 0x55e48073f9ee 0x55e4806d2bda 0x55e480740c0d 0x55e48073fced 0x55e4806d2bda 0x55e480740c0d 0x55e4806d2afa 0x55e480740c0d 0x55e48073f9ee 0x55e4806d3271 0x55e4806d3698 0x55e480741fe4 0x55e48073f9ee 0x55e4806d2bda 0x55e480740915\n",
            "\n",
            "Training options:\n",
            "{\n",
            "  \"G_args\": {\n",
            "    \"func_name\": \"training.networks.G_main\",\n",
            "    \"fmap_base\": 16384,\n",
            "    \"fmap_max\": 512,\n",
            "    \"mapping_layers\": 2,\n",
            "    \"num_fp16_res\": 4,\n",
            "    \"conv_clamp\": 256\n",
            "  },\n",
            "  \"D_args\": {\n",
            "    \"func_name\": \"training.networks.D_main\",\n",
            "    \"mbstd_group_size\": 4,\n",
            "    \"fmap_base\": 16384,\n",
            "    \"fmap_max\": 512,\n",
            "    \"num_fp16_res\": 4,\n",
            "    \"conv_clamp\": 256\n",
            "  },\n",
            "  \"G_opt_args\": {\n",
            "    \"beta1\": 0.0,\n",
            "    \"beta2\": 0.99,\n",
            "    \"learning_rate\": 0.002\n",
            "  },\n",
            "  \"D_opt_args\": {\n",
            "    \"beta1\": 0.0,\n",
            "    \"beta2\": 0.99,\n",
            "    \"learning_rate\": 0.002\n",
            "  },\n",
            "  \"loss_args\": {\n",
            "    \"func_name\": \"training.loss.stylegan2\",\n",
            "    \"r1_gamma\": 1.0\n",
            "  },\n",
            "  \"augment_args\": {\n",
            "    \"class_name\": \"training.augment.AdaptiveAugment\",\n",
            "    \"tune_heuristic\": \"rt\",\n",
            "    \"tune_target\": 0.6,\n",
            "    \"apply_func\": \"training.augment.augment_pipeline\",\n",
            "    \"apply_args\": {\n",
            "      \"xflip\": 1,\n",
            "      \"rotate90\": 1,\n",
            "      \"xint\": 1,\n",
            "      \"scale\": 1,\n",
            "      \"rotate\": 1,\n",
            "      \"aniso\": 1,\n",
            "      \"xfrac\": 1,\n",
            "      \"brightness\": 1,\n",
            "      \"contrast\": 1,\n",
            "      \"lumaflip\": 1,\n",
            "      \"hue\": 1,\n",
            "      \"saturation\": 1\n",
            "    },\n",
            "    \"tune_kimg\": 100\n",
            "  },\n",
            "  \"num_gpus\": 1,\n",
            "  \"image_snapshot_ticks\": 1,\n",
            "  \"network_snapshot_ticks\": 1,\n",
            "  \"train_dataset_args\": {\n",
            "    \"path\": \"/content/datasets/best_images\",\n",
            "    \"max_label_size\": 0,\n",
            "    \"use_raw\": false,\n",
            "    \"resolution\": 1024,\n",
            "    \"mirror_augment\": true,\n",
            "    \"mirror_augment_v\": false\n",
            "  },\n",
            "  \"metric_arg_list\": [],\n",
            "  \"metric_dataset_args\": {\n",
            "    \"path\": \"/content/datasets/best_images\",\n",
            "    \"max_label_size\": 0,\n",
            "    \"use_raw\": false,\n",
            "    \"resolution\": 1024,\n",
            "    \"mirror_augment\": true,\n",
            "    \"mirror_augment_v\": false\n",
            "  },\n",
            "  \"total_kimg\": 25000,\n",
            "  \"minibatch_size\": 4,\n",
            "  \"minibatch_gpu\": 4,\n",
            "  \"G_smoothing_kimg\": 1.25,\n",
            "  \"G_smoothing_rampup\": null,\n",
            "  \"resume_pkl\": \"https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/transfer-learning-source-nets/ffhq-res1024-mirror-stylegan2-noaug.pkl\",\n",
            "  \"run_dir\": \"/content/drive/MyDrive/Colab Notebooks/results/best_images/00000-best_images-mirror-auto1-gamma1-resumeffhq1024\"\n",
            "}\n",
            "\n",
            "Output directory:  /content/drive/MyDrive/Colab Notebooks/results/best_images/00000-best_images-mirror-auto1-gamma1-resumeffhq1024\n",
            "Training data:     /content/datasets/best_images\n",
            "Training length:   25000 kimg\n",
            "Resolution:        1024\n",
            "Number of GPUs:    1\n",
            "\n",
            "Creating output directory...\n",
            "Loading training set...\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x55e487384000 @  0x7fcaa7ae3001 0x7fcaa4ce654f 0x7fcaa4d36b58 0x7fcaa4d3ab17 0x7fcaa4dd9203 0x55e4806d1544 0x55e4806d1240 0x55e480745627 0x55e48073fced 0x55e4806d348c 0x55e480714159 0x55e4807110a4 0x55e4806d1d49 0x55e48074594f 0x55e48073f9ee 0x55e480611e2b 0x55e480741fe4 0x55e48073f9ee 0x55e480611e2b 0x55e480741fe4 0x55e48073fced 0x55e480611e2b 0x55e480741fe4 0x55e4806d2afa 0x55e480740915 0x55e48073f9ee 0x55e48073f6f3 0x55e4808094c2 0x55e48080983d 0x55e4808096e6 0x55e4807e1163\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x55e78882e000 @  0x7fcaa7ae11e7 0x7fcaa4ce646e 0x7fcaa4d36c7b 0x7fcaa4d3735f 0x7fcaa4dd9103 0x55e4806d1544 0x55e4806d1240 0x55e480745627 0x55e48073f9ee 0x55e4806d2bda 0x55e480741737 0x55e48073f9ee 0x55e4806d2bda 0x55e480741737 0x55e48073f9ee 0x55e4806d2bda 0x55e480741737 0x55e4806d2afa 0x55e480740915 0x55e48073f9ee 0x55e4806d2bda 0x55e480744d00 0x55e48073f9ee 0x55e4806d2bda 0x55e480741737 0x55e48073fced 0x55e4806d348c 0x55e480714159 0x55e4807110a4 0x55e4806d1d49 0x55e48074594f\n",
            "tcmalloc: large alloc 4294967296 bytes == 0x55e78882e000 @  0x7fcaa7ae11e7 0x7fcaa4ce646e 0x7fcaa4d36c7b 0x7fcaa4d3735f 0x7fca506e6235 0x7fca50069792 0x7fca50069d42 0x7fca50022aee 0x55e4806d1437 0x55e4806d1240 0x55e4807450f3 0x55e4806d2afa 0x55e480740c0d 0x55e48073fced 0x55e480611eb0 0x55e480741fe4 0x55e48073f9ee 0x55e4806d2bda 0x55e480740c0d 0x55e48073fced 0x55e4806d2bda 0x55e480740c0d 0x55e4806d2afa 0x55e480740c0d 0x55e48073f9ee 0x55e4806d3271 0x55e4806d3698 0x55e480741fe4 0x55e48073f9ee 0x55e4806d2bda 0x55e480740915\n",
            "Image shape: [3, 1024, 1024]\n",
            "Label shape: [0]\n",
            "\n",
            "Constructing networks...\n",
            "Setting up TensorFlow plugin \"fused_bias_act.cu\": Compiling... Loading... Done.\n",
            "Setting up TensorFlow plugin \"upfirdn_2d.cu\": Compiling... Loading... Done.\n",
            "Resuming from \"https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/transfer-learning-source-nets/ffhq-res1024-mirror-stylegan2-noaug.pkl\"\n",
            "Downloading https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/transfer-learning-source-nets/ffhq-res1024-mirror-stylegan2-noaug.pkl ... done\n",
            "\n",
            "G                               Params    OutputShape          WeightShape     \n",
            "---                             ---       ---                  ---             \n",
            "latents_in                      -         (?, 512)             -               \n",
            "labels_in                       -         (?, 0)               -               \n",
            "epochs                          1         ()                   ()              \n",
            "epochs_1                        1         ()                   ()              \n",
            "G_mapping/Normalize             -         (?, 512)             -               \n",
            "G_mapping/Dense0                262656    (?, 512)             (512, 512)      \n",
            "G_mapping/Dense1                262656    (?, 512)             (512, 512)      \n",
            "G_mapping/Broadcast             -         (?, 18, 512)         -               \n",
            "dlatent_avg                     -         (512,)               -               \n",
            "Truncation/Lerp                 -         (?, 18, 512)         -               \n",
            "G_synthesis/4x4/Const           8192      (?, 512, 4, 4)       (1, 512, 4, 4)  \n",
            "G_synthesis/4x4/Conv            2622465   (?, 512, 4, 4)       (3, 3, 512, 512)\n",
            "G_synthesis/4x4/ToRGB           264195    (?, 3, 4, 4)         (1, 1, 512, 3)  \n",
            "G_synthesis/8x8/Conv0_up        2622465   (?, 512, 8, 8)       (3, 3, 512, 512)\n",
            "G_synthesis/8x8/Conv1           2622465   (?, 512, 8, 8)       (3, 3, 512, 512)\n",
            "G_synthesis/8x8/Upsample        -         (?, 3, 8, 8)         -               \n",
            "G_synthesis/8x8/ToRGB           264195    (?, 3, 8, 8)         (1, 1, 512, 3)  \n",
            "G_synthesis/16x16/Conv0_up      2622465   (?, 512, 16, 16)     (3, 3, 512, 512)\n",
            "G_synthesis/16x16/Conv1         2622465   (?, 512, 16, 16)     (3, 3, 512, 512)\n",
            "G_synthesis/16x16/Upsample      -         (?, 3, 16, 16)       -               \n",
            "G_synthesis/16x16/ToRGB         264195    (?, 3, 16, 16)       (1, 1, 512, 3)  \n",
            "G_synthesis/32x32/Conv0_up      2622465   (?, 512, 32, 32)     (3, 3, 512, 512)\n",
            "G_synthesis/32x32/Conv1         2622465   (?, 512, 32, 32)     (3, 3, 512, 512)\n",
            "G_synthesis/32x32/Upsample      -         (?, 3, 32, 32)       -               \n",
            "G_synthesis/32x32/ToRGB         264195    (?, 3, 32, 32)       (1, 1, 512, 3)  \n",
            "G_synthesis/64x64/Conv0_up      2622465   (?, 512, 64, 64)     (3, 3, 512, 512)\n",
            "G_synthesis/64x64/Conv1         2622465   (?, 512, 64, 64)     (3, 3, 512, 512)\n",
            "G_synthesis/64x64/Upsample      -         (?, 3, 64, 64)       -               \n",
            "G_synthesis/64x64/ToRGB         264195    (?, 3, 64, 64)       (1, 1, 512, 3)  \n",
            "G_synthesis/128x128/Conv0_up    1442561   (?, 256, 128, 128)   (3, 3, 512, 256)\n",
            "G_synthesis/128x128/Conv1       721409    (?, 256, 128, 128)   (3, 3, 256, 256)\n",
            "G_synthesis/128x128/Upsample    -         (?, 3, 128, 128)     -               \n",
            "G_synthesis/128x128/ToRGB       132099    (?, 3, 128, 128)     (1, 1, 256, 3)  \n",
            "G_synthesis/256x256/Conv0_up    426369    (?, 128, 256, 256)   (3, 3, 256, 128)\n",
            "G_synthesis/256x256/Conv1       213249    (?, 128, 256, 256)   (3, 3, 128, 128)\n",
            "G_synthesis/256x256/Upsample    -         (?, 3, 256, 256)     -               \n",
            "G_synthesis/256x256/ToRGB       66051     (?, 3, 256, 256)     (1, 1, 128, 3)  \n",
            "G_synthesis/512x512/Conv0_up    139457    (?, 64, 512, 512)    (3, 3, 128, 64) \n",
            "G_synthesis/512x512/Conv1       69761     (?, 64, 512, 512)    (3, 3, 64, 64)  \n",
            "G_synthesis/512x512/Upsample    -         (?, 3, 512, 512)     -               \n",
            "G_synthesis/512x512/ToRGB       33027     (?, 3, 512, 512)     (1, 1, 64, 3)   \n",
            "G_synthesis/1024x1024/Conv0_up  51297     (?, 32, 1024, 1024)  (3, 3, 64, 32)  \n",
            "G_synthesis/1024x1024/Conv1     25665     (?, 32, 1024, 1024)  (3, 3, 32, 32)  \n",
            "G_synthesis/1024x1024/Upsample  -         (?, 3, 1024, 1024)   -               \n",
            "G_synthesis/1024x1024/ToRGB     16515     (?, 3, 1024, 1024)   (1, 1, 32, 3)   \n",
            "---                             ---       ---                  ---             \n",
            "Total                           28794126                                       \n",
            "\n",
            "\n",
            "D                     Params    OutputShape          WeightShape     \n",
            "---                   ---       ---                  ---             \n",
            "images_in             -         (?, 3, 1024, 1024)   -               \n",
            "labels_in             -         (?, 0)               -               \n",
            "1024x1024/FromRGB     128       (?, 32, 1024, 1024)  (1, 1, 3, 32)   \n",
            "1024x1024/Conv0       9248      (?, 32, 1024, 1024)  (3, 3, 32, 32)  \n",
            "1024x1024/Conv1_down  18496     (?, 64, 512, 512)    (3, 3, 32, 64)  \n",
            "1024x1024/Skip        2048      (?, 64, 512, 512)    (1, 1, 32, 64)  \n",
            "512x512/Conv0         36928     (?, 64, 512, 512)    (3, 3, 64, 64)  \n",
            "512x512/Conv1_down    73856     (?, 128, 256, 256)   (3, 3, 64, 128) \n",
            "512x512/Skip          8192      (?, 128, 256, 256)   (1, 1, 64, 128) \n",
            "256x256/Conv0         147584    (?, 128, 256, 256)   (3, 3, 128, 128)\n",
            "256x256/Conv1_down    295168    (?, 256, 128, 128)   (3, 3, 128, 256)\n",
            "256x256/Skip          32768     (?, 256, 128, 128)   (1, 1, 128, 256)\n",
            "128x128/Conv0         590080    (?, 256, 128, 128)   (3, 3, 256, 256)\n",
            "128x128/Conv1_down    1180160   (?, 512, 64, 64)     (3, 3, 256, 512)\n",
            "128x128/Skip          131072    (?, 512, 64, 64)     (1, 1, 256, 512)\n",
            "64x64/Conv0           2359808   (?, 512, 64, 64)     (3, 3, 512, 512)\n",
            "64x64/Conv1_down      2359808   (?, 512, 32, 32)     (3, 3, 512, 512)\n",
            "64x64/Skip            262144    (?, 512, 32, 32)     (1, 1, 512, 512)\n",
            "32x32/Conv0           2359808   (?, 512, 32, 32)     (3, 3, 512, 512)\n",
            "32x32/Conv1_down      2359808   (?, 512, 16, 16)     (3, 3, 512, 512)\n",
            "32x32/Skip            262144    (?, 512, 16, 16)     (1, 1, 512, 512)\n",
            "16x16/Conv0           2359808   (?, 512, 16, 16)     (3, 3, 512, 512)\n",
            "16x16/Conv1_down      2359808   (?, 512, 8, 8)       (3, 3, 512, 512)\n",
            "16x16/Skip            262144    (?, 512, 8, 8)       (1, 1, 512, 512)\n",
            "8x8/Conv0             2359808   (?, 512, 8, 8)       (3, 3, 512, 512)\n",
            "8x8/Conv1_down        2359808   (?, 512, 4, 4)       (3, 3, 512, 512)\n",
            "8x8/Skip              262144    (?, 512, 4, 4)       (1, 1, 512, 512)\n",
            "4x4/MinibatchStddev   -         (?, 513, 4, 4)       -               \n",
            "4x4/Conv              2364416   (?, 512, 4, 4)       (3, 3, 513, 512)\n",
            "4x4/Dense0            4194816   (?, 512)             (8192, 512)     \n",
            "Output                513       (?, 1)               (512, 1)        \n",
            "---                   ---       ---                  ---             \n",
            "Total                 29012513                                       \n",
            "\n",
            "Exporting sample images...\n",
            "Replicating networks across 1 GPUs...\n",
            "Initializing augmentations...\n",
            "Setting up optimizers...\n",
            "Constructing training graph...\n",
            "Finalizing training ops...\n",
            "Initializing metrics...\n",
            "Training for 25000 kimg...\n",
            "\n",
            "tick 0     kimg 0.0      time 2m 20s       sec/tick 26.1    sec/kimg 1632.61 maintenance 114.2  gpumem 10.1  augment 0.000\n",
            "tick 1     kimg 4.0      time 34m 53s      sec/tick 1934.4  sec/kimg 483.59  maintenance 18.1   gpumem 10.1  augment 0.034\n",
            "tick 2     kimg 8.0      time 1h 07m 20s   sec/tick 1943.7  sec/kimg 485.93  maintenance 3.8    gpumem 10.1  augment 0.069\n",
            "tick 3     kimg 12.0     time 1h 39m 54s   sec/tick 1950.2  sec/kimg 487.55  maintenance 3.7    gpumem 10.1  augment 0.104\n",
            "tick 4     kimg 16.0     time 2h 12m 37s   sec/tick 1958.9  sec/kimg 489.73  maintenance 3.7    gpumem 10.1  augment 0.135\n",
            "tick 5     kimg 20.0     time 2h 45m 24s   sec/tick 1963.8  sec/kimg 490.96  maintenance 3.7    gpumem 10.1  augment 0.167\n",
            "tick 6     kimg 24.0     time 3h 18m 20s   sec/tick 1971.2  sec/kimg 492.80  maintenance 4.9    gpumem 10.1  augment 0.197\n",
            "tick 7     kimg 28.0     time 3h 51m 22s   sec/tick 1977.2  sec/kimg 494.31  maintenance 4.1    gpumem 10.1  augment 0.228\n",
            "tick 8     kimg 32.0     time 4h 24m 26s   sec/tick 1980.5  sec/kimg 495.13  maintenance 3.8    gpumem 10.1  augment 0.255\n",
            "tick 9     kimg 36.0     time 4h 57m 35s   sec/tick 1985.0  sec/kimg 496.25  maintenance 3.8    gpumem 10.1  augment 0.281\n",
            "tick 10    kimg 40.0     time 5h 30m 49s   sec/tick 1990.6  sec/kimg 497.65  maintenance 4.0    gpumem 10.1  augment 0.306\n",
            "tick 11    kimg 44.0     time 6h 04m 07s   sec/tick 1993.1  sec/kimg 498.28  maintenance 3.9    gpumem 10.1  augment 0.330\n",
            "tick 12    kimg 48.0     time 6h 37m 28s   sec/tick 1998.1  sec/kimg 499.52  maintenance 3.9    gpumem 10.1  augment 0.354\n",
            "tick 13    kimg 52.0     time 7h 10m 52s   sec/tick 1999.2  sec/kimg 499.81  maintenance 3.9    gpumem 10.1  augment 0.375\n",
            "tick 14    kimg 56.0     time 7h 44m 20s   sec/tick 2004.6  sec/kimg 501.15  maintenance 3.9    gpumem 10.1  augment 0.396\n",
            "tick 15    kimg 60.0     time 8h 17m 50s   sec/tick 2006.0  sec/kimg 501.50  maintenance 4.1    gpumem 10.1  augment 0.414\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TJLghlaalL_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14be3e07-4238-44e6-f2a0-a3a390838684"
      },
      "source": [
        "from PIL import Image\n",
        "\n",
        "img = Image.open('/content/drive/MyDrive/Colab Notebooks/results/best_images/00000-best_images-mirror-auto1-gamma1-resumeffhq1024/fakes000060.jpg')\n",
        "\n",
        "%cd /content/drive/MyDrive/Colab Notebooks/results\n",
        "%mkdir crop_images\n",
        "\n",
        "for n in range(4):\n",
        "  imgNum  = 1 + (7*n)\n",
        "  x, y = 0, (0 + 1024*n)\n",
        "  w, h = 1024, (1024 + 1024*n)\n",
        "  for i in range(1,8):\n",
        "    try:\n",
        "        area = (x,y,w,h)\n",
        "        cropped = img.crop(area)\n",
        "        cropped.save(\"/content/drive/MyDrive/Colab Notebooks/results/crop_images/Test_{}.png\".format(imgNum),'PNG')\n",
        "        imgNum += 1\n",
        "        x += 1024\n",
        "        w += 1024\n",
        "    except:\n",
        "      continue"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/results\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCaFY0WAf3cp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}